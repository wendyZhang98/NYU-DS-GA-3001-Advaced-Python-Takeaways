{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 12\n",
    "## Introduction to GPUs (Graphics Processing Units)\n",
    "### Apr. 26, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of this lecture is based on the previous materials, see:\n",
    "\n",
    "https://nyu-cds.github.io/python-gpu/\n",
    "\n",
    "https://nyu-cds.github.io/python-numba/05-cuda/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "A central processing unit (CPU) is designed to handle complex tasks, emulating virtual machines, control complex flows and branching, security, etc. In contrast, graphical processing units (GPUs) only do one thing well, namely, to handle billions of repetitive low level tasks.\n",
    "\n",
    "---\n",
    "\n",
    "Originally designed for the **rendering of triangles** in 3D graphics:\n",
    "[https://en.wikipedia.org/wiki/Triangle_mesh](https://en.wikipedia.org/wiki/Triangle_mesh)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**GPU**s have 1000s of **arithmetic logic units** (ALUs) compared with traditional CPUs that commonly have only 4 or 8. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Many types of scientific algorithms spend most of their time doing just what GPUs are good for: performing billions of repetitive arithmetic operations.\n",
    "\n",
    "\n",
    "<img src=\"cpugpuarch.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The following diagram shows how GPU performance has increased compared to traditional CPU architetures along the years.\n",
    "\n",
    "<img src=\"01-flops.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Difference between a CPU and a GPU\n",
    "This [video](https://www.youtube.com/watch?v=-P28LKWTzrI) is a funny illustration of the difference, in terms of processing capability, between CPUs and GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "When computer scientists first attempted to use GPUs for scientific computing, the scientific codes had to be mapped onto operations designed to render triangles. This was incredibly difficult to do, and took a lot of time and dedication. \n",
    "\n",
    "---\n",
    "\n",
    "Nowadays, there are **high level languages** (such as **CUDA** and **OpenCL**) that target the GPUs directly, so GPU programming is rapidly becoming mainstream in the scientific community.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\"OpenCL is an open standard maintained by the non-profit technology consortium Khronos Group. Conformant implementations are available from Altera, AMD, Apple (OpenCL along with OpenGL is deprecated for Apple hardware, in favor of Metal), ARM, Creative, IBM, Imagination, Intel, Nvidia, Qualcomm, Samsung, Vivante, Xilinx, and ZiiLABS.\"\n",
    "\n",
    "CUDA is only implemented by Nvidia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A **GPU program** comprises two parts: \n",
    "1. a *host part* that runs on the CPU: sets up the **parameters** and **data** for the computation\n",
    "2. one or more *kernels* that run on the GPU: perform the **actual computation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "\n",
    "## CUDA Programming\n",
    "\n",
    "The CUDA parallel programming model has **three key abstractions** at its core:\n",
    "- a hierarchy of thread groups\n",
    "- shared memories\n",
    "- barrier synchronization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Granularity** in parallel programming: amount of computation vs communication.\n",
    "* **Fine-grained**: individual tasks are relatively small in terms of code size and execution time. The data is transferred among processors frequently in amounts of one or a few memory words.\n",
    "* **Coarse-grained**: data is communicated infrequently, after larger amounts of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The CUDA abstractions:\n",
    "* fine-grained data parallelism and thread parallelism (thread blocks)\n",
    "* coarse-grained data parallelism and task parallelism (grid)\n",
    "\n",
    "They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- A kernel is executed in parallel by an array of threads:\n",
    "    - All threads run the same code.\n",
    "    - Each thread has an ID that it uses to compute memory addresses and make control decisions.\n",
    "\n",
    "- Threads are arranged as a grid of thread blocks:\n",
    "    - Different grid/block can have different kernels  \n",
    "    - Threads from the same block have access to a shared memory and their execution can be synchronized\n",
    "\n",
    "<img src=\"threadgrid.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Thread blocks are required to execute independently: \n",
    "    - It must be possible to execute them in any order, in parallel or in series \n",
    "    - Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses.\n",
    "    - The grid of blocks and the thread blocks can be 1, 2, or 3-dimensional.\n",
    "\n",
    "<img src=\"threadmapping.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "CUDA is designed for a specific GPU architecture, namely NVIDIA’s Streaming Multiprocessors (SM). \n",
    "- Each SM has:\n",
    "    - a set of execution units\n",
    "    - a set of registers \n",
    "    - a chunk of shared memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"sm.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "In an NVIDIA GPU, the basic unit of execution is the __warp__. A warp is a collection of threads, 32 in current implementations, that are executed simultaneously by an SM. Multiple warps can be executed on an SM at once.\n",
    "\n",
    "When a CUDA program on the host CPU invokes a kernel grid, the blocks of the grid are enumerated and distributed to SMs with available execution capacity. The threads of a thread block execute concurrently on one SM, and multiple thread blocks can execute concurrently on one SM. As thread blocks terminate, new blocks are launched on the vacated SMs.\n",
    "\n",
    "The mapping between warps and thread blocks can affect the performance of the kernel. It is usually a good idea to keep the size of a thread block a multiple of 32 in order to avoid this as much as possible.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thread Identity\n",
    "The index of a thread and its thread ID relate to each other as follows:\n",
    "- For a 1-dimensional block, the thread index and thread ID are the same\n",
    "- For a 2-dimensional block, the thread index (x,y) has thread ID=x+yDx, for block size (Dx,Dy)\n",
    "- For a 3-dimensional block, the thread index (x,y,x) has thread ID=x+yDx+zDxDy, for block size (Dx,Dy,Dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**When a kernel is started, the number of blocks per grid and the number of threads per block are fixed (gridDim and blockDim)**. CUDA makes four pieces of information available to each thread:\n",
    "- The thread index (threadIdx)\n",
    "- The block index (blockIdx)\n",
    "- The size and shape of a block (blockDim)\n",
    "- The size and shape of a grid (gridDim)\n",
    "\n",
    "Typically, each thread in a kernel will compute one element of an array. There is a common pattern to do this that most CUDA programs use are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CUDA simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not have a CUDA-enabled GPU on your system, \n",
    "# you will receive one of the following errors:\n",
    "\n",
    "# numba.cuda.cudadrv.error.CudaDriverError: CUDA initialized before forking\n",
    "# CudaSupportError: Error at driver init: \n",
    "# [3] Call to cuInit results in CUDA_ERROR_NOT_INITIALIZED:\n",
    "# numba.cuda.cudadrv.error.CudaDriverError: Error at driver init:\n",
    "# CUDA disabled by user:\n",
    "# If you do have a CUDA-enabled GPU on your system, you should see a message like:\n",
    "\n",
    "# <Managed Device 0>\n",
    "# If your machine has multiple GPUs, you might want to select which one to use. \n",
    "# By default the CUDA driver selects the fastest GPU as the device 0, \n",
    "# which is the default device used by Numba.\n",
    "\n",
    "# numba.cuda.select_device( device_id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the CUDA simulator\n",
    "# If you don’t have a CUDA-enabled GPU \n",
    "# (i.e. you received one of the error messages described previously), \n",
    "# then you will need to use the CUDA simulator. \n",
    "# The simulator is enabled by setting the environment variable \n",
    "# NUMBA_ENABLE_CUDASIM to 1.\n",
    "\n",
    "\n",
    "# Mac/Linux\n",
    "# Launch a terminal shell and type the commands:\n",
    "!export NUMBA_ENABLE_CUDASIM=1\n",
    "\n",
    "# Windows\n",
    "# Launch a CMD shell and type the commands:\n",
    "# SET NUMBA_ENABLE_CUDASIM=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "print(cuda.gpus)\n",
    "\n",
    "cuda.select_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%writefile cuda01.py\n",
    "\n",
    "from __future__ import division\n",
    "from numba import cuda\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    bw = cuda.blockDim.x\n",
    "    \n",
    "    index = tx + bx * bw\n",
    "    io_array[index] = index * 10\n",
    "    print(\"i, t, b, w:\", index, tx, bx, bw)\n",
    "        \n",
    "        \n",
    "# Host code   \n",
    "data = numpy.ones(256)\n",
    "threadsperblock = 16\n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "print(\"\\ndata:\\n\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### For a 2-dimensional grid:\n",
    "# tx = cuda.threadIdx.x\n",
    "# ty = cuda.threadIdx.y\n",
    "# bx = cuda.blockIdx.x\n",
    "# by = cuda.blockIdx.y\n",
    "# bw = cuda.blockDim.x\n",
    "# bh = cuda.blockDim.y\n",
    "# x = tx + bx * bw\n",
    "# y = ty + by * bh\n",
    "# array[x, y] = compute(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Hierarchy and Data Transfer\n",
    "The CPU and GPU have separate memory spaces. This means that data that is processed by the GPU must be moved from the CPU to the GPU before the computation starts, and the results of the computation must be moved back to the CPU once processing has completed.\n",
    "\n",
    "#### Global memory\n",
    "This memory is accessible to __all threads__ as well as the host (CPU).\n",
    "- Global memory is allocated and deallocated by the host\n",
    "- Used to initialize the data that the GPU will work on\n",
    "\n",
    "#### Shared memory\n",
    "__Each thread block__ has its own shared memory\n",
    "- Accessible only by threads within the block\n",
    "- Much faster than local or global memory\n",
    "- Requires special handling to get maximum performance\n",
    "- Only exists for the lifetime of the block\n",
    "\n",
    "#### Local memory\n",
    "__Each thread__ has its own private local memory\n",
    "- Only exists for the lifetime of the thread\n",
    "- Generally handled automatically by the compiler\n",
    "\n",
    "#### Constant and texture memory\n",
    "These are __read-only memory__ spaces accessible by __all threads__.\n",
    "- Constant memory is used to cache values that are shared by all functional units\n",
    "- Texture memory is optimized for texturing operations provided by the hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## OpenCL and pyOpenCL\n",
    "__OpenCL__ (Open Computing Language) is an **open standard** for cross-platform, **parallel programming**. It was originally developed by Apple in 2008 and is now maintained by the Khronos Group.\n",
    "\n",
    "<img src=\"opencl.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    " \n",
    "While OpenCL supports many different types of processors, as for example GPUs, DSPs, and FPGAs, it is most notably used to access the GPU for general computing tasks.\n",
    "\n",
    "__pyOpenCL__ is a package (MIT license) that enables developers to easily access the OpenCL API from Python.\n",
    "\n",
    "A standard and a minimal OpenCL code will have following parts.\n",
    "1. Identifying a Platform\n",
    "2. Finding the device ID\n",
    "3. Creating the context: _to manage objects such as command-queues, memory, program and kernel objects and for executing kernels on one or more devices specified in the context._\n",
    "4. Creating a command queue in the context\n",
    "5. Creating a program source and a kernel entry point\n",
    "6. Creating the buffers for data handling\n",
    "7. Kernel Program\n",
    "8. Build and Launch the Kernel\n",
    "9. Read the Output Buffer and clear it (if needed)\n",
    "\n",
    "A _pyopencl_ user will have its own device identified by environment variables, simplifying things. Examples can be found [here](https://github.com/inducer/pyopencl/tree/master/examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Important:**\n",
    "\n",
    "See and install pocl to get OpenCL device drivers: https://anaconda.org/conda-forge/pocl\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile info.py\n",
    "\n",
    "# Find out about your computer's OpenCL situation\n",
    "import pyopencl as cl  # Import the OpenCL GPU computing API\n",
    "\n",
    "for platform in cl.get_platforms():  # Print each platform on this computer\n",
    "    print('=' * 10)\n",
    "    print('Platform - Name:  ' + platform.name)\n",
    "    print('Platform - Vendor:  ' + platform.vendor)\n",
    "    print('Platform - Version:  ' + platform.version)\n",
    "    print('Platform - Profile:  ' + platform.profile)\n",
    "    \n",
    "    for device in platform.get_devices():  # Print each device per-platform\n",
    "        print('    ' + '-' * 6)\n",
    "        print('    Device - Name:  ' + device.name)\n",
    "        print('    Device - Type:  ' + cl.device_type.to_string(device.type))\n",
    "        print('    Device - Max Clock Speed:  {0} Mhz'.format(device.max_clock_frequency))\n",
    "        print('    Device - Compute Units:  {0}'.format(device.max_compute_units))\n",
    "        print('    Device - Local Memory:  {0:.0f} KB'.format(device.local_mem_size/1024))\n",
    "        print('    Device - Constant Memory:  {0:.0f} KB'.format(device.max_constant_buffer_size/1024))\n",
    "        print('    Device - Global Memory: {0:.0f} GB'.format(device.global_mem_size/1073741824.0))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use OpenCL To Add Two Random Arrays (This Way Hides Details)\n",
    "\n",
    "import pyopencl as cl  # Import the OpenCL GPU computing API\n",
    "import pyopencl.array as pycl_array  # Import PyOpenCL Array \n",
    "#(a Numpy array plus an OpenCL buffer object)\n",
    "\n",
    "import numpy as np  # Import Numpy number tools\n",
    "\n",
    "# platform = cl.get_platforms()[0]  # Select the first platform [0]\n",
    "# device = platform.get_devices()[0]  # Select the first device on this platform [0]\n",
    "# context = cl.Context([device])  # Create a context with your device\n",
    "context = cl.create_some_context()  # Initialize the Context\n",
    "\n",
    "print(context)\n",
    "queue   = cl.CommandQueue(context)  # Instantiate a Queue\n",
    "\n",
    "# Create two random pyopencl arrays\n",
    "a = pycl_array.to_device(queue, np.random.rand(50000).astype(np.float32))\n",
    "b = pycl_array.to_device(queue, np.random.rand(50000).astype(np.float32))  \n",
    "\n",
    "# Create an empty pyopencl destination array\n",
    "res_c = pycl_array.empty_like(a)  \n",
    "\n",
    "program = cl.Program(context, \"\"\"\n",
    "__kernel void sum(__global const float *a, __global const float *b, __global float *c)\n",
    "{\n",
    "  int i = get_global_id(0);\n",
    "  c[i] = a[i] + b[i];\n",
    "}\"\"\").build()  # Create the OpenCL program\n",
    "\n",
    "# Enqueue the program for execution and store the result in c\n",
    "program.sum(queue, a.shape, None, a.data, b.data, res_c.data)  \n",
    "\n",
    "print(\"a: {}\".format(a))\n",
    "print(\"b: {}\".format(b))\n",
    "print(\"c: {}\".format(res_c))  \n",
    "# Print all three arrays, to show sum() worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Example at PyOpenCL's documentation: https://documen.tician.de/pyopencl/\n",
    "\n",
    "\n",
    "See [https://documen.tician.de/pyopencl/runtime_program.html](https://documen.tician.de/pyopencl/runtime_program.html)\n",
    "\n",
    "and *associated memory object* `mem_info`: \n",
    "[https://documen.tician.de/pyopencl/runtime_const.html#mem_info](https://documen.tician.de/pyopencl/runtime_const.html#mem_info)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the same above algorithm but written in a different way\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pyopencl as cl\n",
    "\n",
    "n = 5_000_000\n",
    "\n",
    "a_np = np.random.rand(n).astype(np.float32)\n",
    "b_np = np.random.rand(n).astype(np.float32)\n",
    "\n",
    "# ctx = cl.create_some_context()\n",
    "platform = cl.get_platforms()[0]  # Select the first platform [0]\n",
    "device = platform.get_devices()[1]  # Select the first device on this platform [0]\n",
    "ctx = cl.Context([device])  # Create a context with your device\n",
    "print(ctx)\n",
    "queue = cl.CommandQueue(ctx)\n",
    "\n",
    "# Buffer: class pyopencl.Buffer(context, flags, size=0, hostbuf=None)\n",
    "\n",
    "mf = cl.mem_flags\n",
    "a_g = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=a_np)\n",
    "b_g = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=b_np)\n",
    "\n",
    "\n",
    "# get_global_id\n",
    "# Returns the unique global work-item ID value \n",
    "# for dimension identified by dimindx.\n",
    "\n",
    "prg = cl.Program(ctx, \"\"\"\n",
    "__kernel void sum(\n",
    "    __global const float *a_g, __global const float *b_g, __global float *res_g)\n",
    "{\n",
    "  int gid = get_global_id(0);\n",
    "  res_g[gid] = a_g[gid] + b_g[gid];\n",
    "}\n",
    "\"\"\").build()\n",
    "\n",
    "ts = time()\n",
    "\n",
    "res_g = cl.Buffer(ctx, mf.WRITE_ONLY, a_np.nbytes)\n",
    "prg.sum(queue, a_np.shape, None, a_g, b_g, res_g)\n",
    "\n",
    "res_np = np.empty_like(a_np)\n",
    "cl.enqueue_copy(queue, res_np, res_g)\n",
    "\n",
    "print('Took {}s'.format(time() - ts))\n",
    "\n",
    "# Check on CPU with Numpy:\n",
    "print(a_np[0: 10])\n",
    "print(b_np[0: 10])\n",
    "print(res_np[0: 10])\n",
    "\n",
    "print((res_np - (a_np + b_np))[0:10])\n",
    "print(np.linalg.norm(res_np - (a_np + b_np)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "pyOpenCL has two goals:\n",
    "- Make OpenCL seem simple\n",
    "- Expose OpenCL's complex features\n",
    "\n",
    "Comparing the two previous codes we see that\n",
    "```python\n",
    "context = cl.create_some_context()\n",
    "```\n",
    "is simple, but if you have two GPUs in your computer, this function might select the wrong one.  Therefore, you might want to write three lines instead of one:\n",
    "```python\n",
    "platform = cl.get_platforms()[0]  # Select the first platform [0]\n",
    "device = platform.get_devices()[0]  # Select the first device on this platform [0]\n",
    "context = cl.Context([device])  # Create a context with your device\n",
    "```\n",
    "This second way of creating a context is longer, but it allows you to target the exact platform and device you want to use on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

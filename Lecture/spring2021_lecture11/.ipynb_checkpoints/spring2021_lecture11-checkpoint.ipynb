{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Parallel Computing - Part 02\n",
    "\n",
    "### Lecture 11\n",
    "\n",
    "### Apr 12, 2021\n",
    "\n",
    "Based on material here: https://nyu-cds.github.io/python-mpi/ (parts 4/5).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Problem Decomposition\n",
    "A central problem when working with MPI is to break the problem into “chunks” to be handled by individual processes.\n",
    "\n",
    "There are two main ways to decompose a problem: \n",
    "- __Domain decomposition:__ Data associated with a problem is split into chunks and each parallel process works on a chunk of the data.\n",
    "- __Functional decomposition:__ Focus is on the computation rather than on the data. Used when pieces of data require different processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src=\"domain_decomp.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"funct_decomp.png\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Example of a domain decompostion with MPI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Hermite interpolation:**\n",
    "\n",
    "Concretelly, for example, we want to interpolate $$p(x) = ax^3+bx^2+cx+d$$ where $$p'(x) = 3ax^2 + 2b x + c$$ \n",
    "and have the following estimates: $y_0 = p(x_0), y_1 = p(x_1)$ and $m_0 = p'(x_0), m_1 = p'(x_1)$.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img8.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "ax_0^3+bx_0^2+cx_0+d = y_0\\\\\n",
    "ax_1^3+bx_1^2+cx_1+d = y_1\\\\\n",
    "3ax_0^2+2bx_0+c = m_0\\\\\n",
    "3ax_1^2+2bx_1+c = m_1\n",
    "\\end{array}\n",
    "\\Longrightarrow\n",
    "\\left[\\begin{array}{cccc}\n",
    "x_0^3 & x_0^2 & x_0 & 1\\\\\n",
    "x_1^3 & x_1^2 & x_1 & 1\\\\\n",
    "3x_0^2 & 2x_0 & 1 & 0\\\\\n",
    "3x_1^2 & 2x_1 & 1 & 0\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\n",
    "a\\\\\n",
    "b\\\\\n",
    "c\\\\\n",
    "d\n",
    "\\end{array}\\right]=\n",
    "\\left[\\begin{array}{c}\n",
    "y_0\\\\\n",
    "y_1\\\\\n",
    "m_0\\\\\n",
    "m_1\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hermite(data):\n",
    "    \"\"\"\n",
    "    data in the form: [x[i], x[i+1], y[i], y[i+1], m[i], m[i+1]]\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.zeros((4,4))\n",
    "    pts = []\n",
    "\n",
    "    #create A and b for the linear system\n",
    "    A[0:2] = np.array([np.power(data[0:2], 3), np.power(data[0:2], 2), data[0:2],np.array([1,1])]).T\n",
    "    A[2:4] = np.array([3*np.power(data[0:2],2), 2*data[0:2],np.array([1,1]), np.array([0,0])]).T\n",
    "    b = data[2: ]\n",
    "    \n",
    "    #solve the linear system A * coefs = b \n",
    "    coefs = np.linalg.solve(A, b)\n",
    "    \n",
    "    # interpolate polynomial on [x[i], x[i+1]] in 100 points\n",
    "    t = np.linspace(data[0], data[1], 100)\n",
    "    pts.extend(np.polyval(coefs, t))\n",
    "    \n",
    "    return pts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Finding the piece-wise hermite polynomial interpolation of a set of points\n",
    "# Sequential version\n",
    "#####\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ts = time()\n",
    "numx = 10000   # number of discretized points in x\n",
    "\n",
    "x = np.linspace(0, 20, numx)\n",
    "y = np.random.uniform(low=-1,high=1, size=numx) + 100 * np.sin(x)\n",
    "m = np.random.uniform(low=-1, high=1, size=numx) + 100 * np.cos(x)\n",
    "\n",
    "l = []\n",
    "for i in range(numx - 1):\n",
    "    data = np.array([x[i], x[i+1], y[i], y[i+1], m[i], m[i+1]])\n",
    "    l.extend(hermite(data))\n",
    "   \n",
    "\n",
    "print('Took {}s'.format(time() - ts))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y)\n",
    "\n",
    "s = np.linspace(0, 20, len(l))\n",
    "plt.figure()\n",
    "plt.plot(s, l)\n",
    "\n",
    "plt.figure()\n",
    "end = 15\n",
    "plt.plot(s[0:100*(end-1)], l[0:100*(end-1)])\n",
    "plt.plot(x[0:end], y[0:end])\n",
    "plt.plot(x[0:end], y[0:end],'o')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi08.py\n",
    "#####\n",
    "# Finding the piece-wise hermite polynomial interpolation of a set of points\n",
    "# MPI version\n",
    "#####\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "status = MPI.Status()\n",
    "\n",
    "ts = time()\n",
    "numx = 10_000\n",
    "rank_interval_size = 20 / size\n",
    "numx_per_rank = int(np.floor(numx / size))\n",
    "\n",
    "m = np.zeros((numx_per_rank,))\n",
    "\n",
    "def hermite(data):\n",
    "    A = np.zeros((4, 4))\n",
    "    pts = []\n",
    "\n",
    "    A[0:2]=np.array([np.power(data[0:2], 3),np.power(data[0:2],2),data[0:2],np.array([1,1])]).T\n",
    "    A[2:4]=np.array([3*np.power(data[0:2],2),2*data[0:2],np.array([1,1]),np.array([0,0])]).T\n",
    "    b = data[2:]\n",
    "    coefs = np.linalg.solve(A,b)\n",
    "    t = np.linspace(data[0], data[1], 100)\n",
    "    pts.extend(np.polyval(coefs,t))\n",
    "    return(pts)\n",
    "\n",
    "\n",
    "if rank == 0:\n",
    "    l = []\n",
    "    x = np.linspace(0, rank_interval_size, numx_per_rank)\n",
    "    y = np.random.uniform(low=-1, high=1, size=numx_per_rank) + 100 * np.sin(x)\n",
    "    for i in range(numx_per_rank-1):\n",
    "        data = np.array([x[i], x[i+1], y[i], y[i+1], m[i], m[i+1]])\n",
    "        l.extend(hermite(data))\n",
    "        \n",
    "    # Process 0 serves also as receiver\n",
    "    pts = np.zeros((np.asarray(l).shape[0], ))\n",
    "    for p in range(1, size):\n",
    "        comm.Recv(pts, source=p)\n",
    "        l.extend(np.ndarray.tolist(pts))\n",
    "        \n",
    "    np.save('mpi_hermite_cpt.npy', np.asarray([x, y]))\n",
    "    np.save('mpi_hermite.npy', np.asarray(l))\n",
    "\n",
    "else:\n",
    "    l = []\n",
    "    x = np.linspace(rank*rank_interval_size,(rank + 1)*rank_interval_size, numx_per_rank)\n",
    "    y = np.random.uniform(low=-1,high=1, size=numx_per_rank) + 100 * np.sin(x)\n",
    "    for i in range(numx_per_rank-1):\n",
    "        data = np.array([x[i], x[i+1], y[i], y[i+1], m[i], m[i+1]])\n",
    "        l.extend(hermite(data))\n",
    "    \n",
    "    pts = np.asarray(l) \n",
    "    comm.Send(pts,dest=0)\n",
    "    \n",
    "print('Rank ',rank,' Took {}s'.format(time() - ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -n 4 python3 mpi08.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctp = np.load('mpi_hermite_cpt.npy')\n",
    "r = np.load('mpi_hermite.npy')\n",
    "print(ctp.shape, r.shape)\n",
    "\n",
    "s = np.linspace(0, 20, r.shape[0])\n",
    "plt.figure(1)\n",
    "plt.plot(s, r)\n",
    "plt.figure(2)\n",
    "plt.plot(s[0:500],r[0:500])\n",
    "plt.plot(ctp[0,0:6], ctp[1,0:6])\n",
    "plt.plot(ctp[0,0:6], ctp[1,0:6],'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Collective Operations: point-to-point vs collective communication\n",
    "\n",
    "__Collective communication__ allows to send data between multiple processes of a group simultaneously and the most common operations are: \n",
    "\n",
    "- **Synchronization**\n",
    "  - Processes wait until all members of the group have reached the synchronization point\n",
    "- **Global communication functions**\n",
    "  - Broadcast data from one member to all members of a group\n",
    "  - Gather data from all members to one member of a group\n",
    "  - Scatter data from one member to all members of a group\n",
    "- **Collective computation (reductions)**\n",
    "  - One member of the group collects data from the other members and performs an operation (min, max, add, multiply, etc.) on that data.\n",
    "- **Collective Input/Output**\n",
    "  - Each member of the group reads or writes a section of a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Synchronization**\n",
    "MPI has a special function that is dedicated to synchronizing processes: __comm.Barrier()__. No process advances until all have called the function.\n",
    "$$\n",
    "\\begin{array}{cc|c}\n",
    "& \\bigcirc & \\\\\n",
    "\\bigcirc& \t & \\\\\n",
    "& \t\\bigcirc & \n",
    "\\end{array}\n",
    "\\Rightarrow\n",
    "\\begin{array}{cc|c}\n",
    "& \\bigcirc & \\\\\n",
    "& \\bigcirc\t & \\\\\n",
    "& \t\\bigcirc & \n",
    "\\end{array}\n",
    "\\Rightarrow\n",
    "\\begin{array}{cc|c}\n",
    "& &\\bigcirc  \\\\\n",
    "& &\\bigcirc\t  \\\\\n",
    "& &\t\\bigcirc  \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "<img src=\"comm_barrier.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Global communication functions\n",
    "\n",
    "For detailed diagrams see: [https://nyu-cds.github.io/python-mpi/05-collectives/](https://nyu-cds.github.io/python-mpi/05-collectives/)\n",
    "\n",
    "Examples here: https://mpi4py.readthedocs.io/en/stable/tutorial.html#collective-communication\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### **Broadcasting**\n",
    "One process sends the same data to all processes in a communicator using the command __comm.Bcast(buf, root=0)__.\n",
    "\n",
    "\n",
    "<img src=\"broadcast.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "#### **Scatter**\n",
    "Broadcast sends the same piece of data to all processes while scatter sends chunks of an array to different processes. \n",
    "\n",
    "\n",
    "<img src=\"scatter.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "__Comm.Scatter(sendbuf, recvbuf, root=0)__ method takes three arguments. The first is an array of data that resides on the root process. The second parameter is used to hold the received data. The last parameter indicates the root process that is scattering the array of data.\n",
    "\n",
    "\n",
    "#### **Gather**\n",
    "Gather is the inverse of scatter, taking elements from many processes and gathering them to one single process.\n",
    "\n",
    "<img src=\"gather.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "__Comm.Gather(sendbuf, recvbuf, root=0)__ method takes the same arguments as __Comm.Scatter__.\n",
    "\n",
    "\n",
    "#### **Reduction**\n",
    "__Comm.Reduce(sendbuf, recvbuf, op=MPI.SUM, root=0)__ handles almost all of the common reductions that a programmer needs to do in a parallel application.\n",
    "\n",
    "<img src=\"reduction.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "__Comm.Reduce__ takes an array of input elements and returns an array of reduced elements to the root process.\n",
    "- MPI.MAX - Returns the maximum element.\n",
    "- MPI.MIN - Returns the minimum element.\n",
    "- MPI.SUM - Sums the elements.\n",
    "- MPI.PROD - Multiplies all elements.\n",
    "\n",
    "#### **Other Colletive Operations**\n",
    "- __Comm.Alltoall(sendbuf, recvbuf)__\n",
    "- __File.Open(comm, filename, amode, info)__\n",
    "- __File.Write_all(buffer)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Computing an integral using parallel collective version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpi09.py\n",
    "import numpy\n",
    "from math import sin\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "def integral(a_r, h, n):\n",
    "    integ = 0.0\n",
    "    for j in range(n):\n",
    "        t = a_r + (j + 0.5) * h\n",
    "        integ += sin(t) * h\n",
    "    return integ\n",
    "\n",
    "\n",
    "a = 0.0\n",
    "b = numpy.pi / 2\n",
    "dest = 0\n",
    "my_int = numpy.zeros(1)\n",
    "integral_sum = numpy.zeros(1)\n",
    "\n",
    "# Initialize value of n only if this is rank 0\n",
    "if rank == 0:\n",
    "    n = numpy.full(1, 500, dtype=int) # default value\n",
    "else:\n",
    "    n = numpy.zeros(1, dtype=int)\n",
    "\n",
    "# Broadcast n to all processes\n",
    "print(\"Process \", rank, \" before n =\", n[0])\n",
    "comm.Bcast(n, root=0)\n",
    "print(\"Process \", rank, \" after n =\", n[0])\n",
    "\n",
    "# Compute partition\n",
    "h = (b - a) / (n * size) # calculate h *after* we receive n\n",
    "a_r = a + rank * h * n\n",
    "my_int[0] = integral(a_r, h, n[0])\n",
    "\n",
    "# Send partition back to root process, computing sum across all partitions\n",
    "print(\"Process \", rank, \" has the partial integral \", my_int[0])\n",
    "comm.Reduce(my_int, integral_sum, MPI.SUM, dest)\n",
    "\n",
    "# Only print the result in process 0\n",
    "if rank == 0:\n",
    "    print('The Integral Sum =', integral_sum[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -n 4 python mpi09.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-vector multiplication\n",
    "\n",
    "Parallelize $y = Ax$ over the rows of $A$.\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}y_1 \\\\ \\vdots \\\\ y_k \\end{pmatrix} = \\begin{pmatrix}A_1 \\\\ \\vdots \\\\ A_k \\end{pmatrix} x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mpimatvec.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "n = size * 2\n",
    "m = 5\n",
    "if rank == 0:\n",
    "    A = np.random.randn(n, m)\n",
    "    x = np.random.randn(m)\n",
    "\n",
    "    ytarget = A.dot(x) # to check the result\n",
    "    \n",
    "    A = A.reshape(size, -1, m)\n",
    "else:\n",
    "    A = None\n",
    "    x = np.zeros(m)\n",
    "\n",
    "Asmall = np.zeros((n // size, m))\n",
    "\n",
    "# Asmall = comm.scatter(A, root=0)\n",
    "comm.Scatter(A, Asmall, root=0)\n",
    "\n",
    "# x = comm.bcast(x, root=0)\n",
    "comm.Bcast(x, root=0)\n",
    "\n",
    "print('rank', rank, ':', Asmall.shape, x.shape)\n",
    "\n",
    "ysmall = Asmall.dot(x)\n",
    "\n",
    "# y = comm.gather(ysmall, root=0)\n",
    "y = np.zeros((size, n // size))\n",
    "comm.Gather(ysmall, y, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print(np.concatenate(y))\n",
    "    print(ytarget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -n 4 python mpimatvec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.random.randn(4, 5)\n",
    "x = np.random.randn(5)\n",
    "\n",
    "A.dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication of buffer-like objects\n",
    "When using the upper case version of the methods (Send, Irecv, Gather, etc.) the data object must support the **single-segment buffer interface**. This interface is a standard Python mechanism provided by some types (e.g., strings and numeric arrays), which is why we have been using NumPy arrays in the examples.\n",
    "\n",
    "### Communication of generic Python objects\n",
    "It is also possible to transmit an arbitrary Python data type using the lower case version of the methods (send, irecv, gather, etc.) mpi4py will **serialize** the data type, send it to the remote process, then **deserialize** it back to the original data type (a process known as **pickling** and **unpickling**). While this is simple, it also adds significant overhead to the MPI operation."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
